# GitHub
GITHUB_REPO=mmenalla/code-risk-guard
GITHUB_REPOS=["scikit-learn/scikit-learn", "pandas-dev/pandas", "huggingface/transformers", "mmenalla/readlike-me"]
GITHUB_TOKEN=ghp_...

# Jira
JIRA_BASE_URL=https://menallamegi.atlassian.net/
JIRA_USER_EMAIL=menallamegi@gmail.com
JIRA_API_TOKEN=A
JIRA_PROJECT_KEY=SCRUM

# LLM / OpenAI
OPENAI_API_KEY=
# Model
MODEL_NAME=xgboost_risk_model.json

SINCE_DAYS=3
MAX_PRS=300

# Training Pipeline Configuration
# Set to 'false' to skip data collection (fetch, aggregate, feature engineering, labeling)
# and train directly from existing labeled data in MongoDB
FETCH_NEW_DATA=True

# Label Source Filter for Training
# Options: 
#   "all"                  - Use all labeled data (default)
#   "sonarqube"            - Only SonarQube-labeled data (high-quality code metrics)
#   "heuristic"            - Only heuristic-labeled data (GitHub PR patterns)
#   "manager"              - Only manager feedback labels (ground truth)
#   "sonarqube+heuristic"  - Both SonarQube and heuristic (exclude manager feedback)
#   "sonarqube+manager"    - SonarQube and manager feedback
LABEL_SOURCE_FILTER=sonarqube

# Ticket Generation
GENERATE_TICKETS=False
RISK_SCORE_THRESHOLD=0.5
NUM_TICKETS_PER_MODULE=3

# MongoDB
MONGO_URI=mongodb://admin:admin@localhost:27017/risk_model_db?authSource=admin
MONGO_DB=risk_model_db
MONGO_COLLECTION=ticket_drafts
MONGO_PREDICTIONS_COLLECTION=model_predictions
MONGO_FEEDBACK_COLLECTION=risk_feedback
# ============================================
# MCP (Model Context Protocol) Configuration
# ============================================

# Enable MCP mode for intelligent ticket generation
# Set to 'true' to use MCP agent with context, tools, and caching
# Set to 'false' to use legacy simple GPT-4 calls
USE_MCP=true

# GPT-4 model to use for MCP (if enabled)
# Options: gpt-4-turbo-preview, gpt-4, gpt-4-32k
MCP_MODEL=gpt-4-turbo-preview

# Enable tool calling (allows GPT-4 to request additional data)
MCP_ENABLE_TOOLS=true

# Enable caching (stores generated tickets to avoid regeneration)
MCP_ENABLE_CACHING=true

# SonarQube (Use Docker service name for container-to-container communication)
# For local browser access, use http://localhost:9000
SONARQUBE_URL=http://sonarqube:9000
SONARQUBE_TOKEN=
# SONARQUBE_PROJECT_KEYS=["scikit-learn", "pandas-dev", "huggingface-transformers", "readlike-me"]
SONARQUBE_PROJECT_KEYS=["TechDebtGPT", "backend-api", "charts", "tech-debt-api", "tech-debt-services"]

# Choose between GitHub PRs or Local Git Commits
# Set to 'true' to analyze local git commits (no GitHub API needed)
# Set to 'false' to analyze GitHub pull requests (requires GITHUB_TOKEN)
USE_COMMITS=true

# Local Repository Configuration (only used when USE_COMMITS=true)
# For multiple local repos, set REPO_BASE_DIR and REPO_NAMES
# NOTE: When running in Docker, use the container path (mounted in docker-compose.yml)
REPO_BASE_DIR=/repos/TDGPTRepos
REPO_NAMES=["TechDebtGPT", "backend-api", "charts", "tech-debt-api", "tech-debt-services"]

# For single repo (legacy), use REPO_PATH
# REPO_PATH=/repos/readlike-me

# Branch to analyze (default: main)
# Examples: main, master, develop, feature/my-feature
# For single repo (legacy): BRANCH_NAME=dev
# For multiple repos with different branches, use REPO_BRANCHES JSON mapping:
BRANCH_NAME=dev
REPO_BRANCHES={"TechDebtGPT": "dev", "backend-api": "dev", "charts": "main", "tech-debt-api": "dev", "tech-debt-services": "dev"}

# Number of commits to analyze (when USE_COMMITS=true)
# Similar to MAX_PRS for PR-based analysis
MAX_COMMITS=10000